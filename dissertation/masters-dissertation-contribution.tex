%!TEX root = masters-dissertation.tex
\chapter{Meta-Level Reinforcement Learning}
\label{chapter:meta-rl}
Having presented the state of the art on reinforcement learning and its related concepts,
the following sections present our proposed solution.


\section{Parameter Control}
\label{sec:parameter-control}

As we have seen in Section~\ref{sec:rl}, the parameters used in the update rule of reinforcement learning influence the way state values are computed, and ultimately how a policy is generated. 
Therefore, the choice of parameters in reinforcement learning
--- such as $\alpha$ ---
can be crucial to success in learning~\cite{schweighofer2003meta}. 
Consequently, there are different strategies to control and adjust these parameters. 

When an agent does not know much about the environment, it needs to explore the environment with a high learning-rate to be able to quickly estimate the actual values of each state. 
However, a high learning-rate can either prevent the algorithm from converging, or lead to inaccuracies in the computed value of each state (e.g., a local maximum). 
For this reason, after the agent learns something about the environment, it should begin to modulate its learning-rate to ensure that either the state values converges, or that the agent overcomes local maxima. 
% 
Consequently, maintaining a high learning-rate hampers the convergence of the Q-value, and Q-learning implementations often use a decreasing function for $\alpha$ as the policy is being refined. 
A typical way~\cite{schweighofer2003meta} to vary the $\alpha$-value, is to start interactions with a value close to $1$, and then decrease it over time toward $0$.
However, this approach is not effective for dynamic environments, since a drastic change in the environment when the learning-rate is close to $0$ prevents the agent from learning the optimal policy in the changed environment. 



\section{Meta-Level Reasoning in Reinforcement Learning}
\label{sec:meta-reasoning-rl}

The objective of meta-level reasoning is to improve the quality of decision making by explicitly reasoning about the parameters of the decision-making process and deciding how to change these parameters in response to the agent's performance. 
Consequently, an agent needs to obtain information about its own reasoning process to reason effectively at the meta-level. 
In this work, we consider the following processes used by our learning agent at each level of reasoning, and illustrate these levels in Figure~\ref{fig:reasoningOnRL}:

\begin{itemize}
\item \textit{ground-level} refers to the implementation of actions according to the MDP's policy;
\item \textit{object-level} refers to learning the parameters of the MDP and the policy itself;
\item \textit{meta-level} refers to manipulating the learning parameters used at the \textit{object-level};
\end{itemize}

\begin{figure}[ht]
\centering
\includegraphics[width=230px]{images/reasoningOnRL}
\caption{Modeling the meta-level reasoning in reinforcement learning.}
\label{fig:reasoningOnRL}
\end{figure}

We have developed a number of strategies for meta-level reinforcement learning and evaluated them.



\subsection{Meta-Level Reasoning in Q-Learning}
\label{subsec:mlql}

Our approach to meta-level reasoning consists of varying the learning-rate (known as $\alpha-$value)
to allow an agent to handle dynamic environments. 
More concretely, at the meta-level, we apply RL to learn the $\alpha-$value used as the learning-rate for object-level RL. 
In other words, we apply reinforcement learning to control the parameters of reinforcement learning. 

The difference between RL applied at the meta-level and RL applied at the object-level is that, at the object-level, we learn Q-value for the state-action pair, increasing it when we have positive feedback and decreasing it when we have negative feedback. 
Conversely, at the meta-level, what we learn is the $\alpha$-value, by decreasing it when we have positive feedback and increasing it when we have negative feedback --- that is, making mistakes means we need to learn at a faster rate. 
Our approach to \textit{meta-level reinforcement learning} is shown in Algorithm~\ref{alg:meta-rl}. 

\begin{algorithm}
	\caption{Meta-Level Reasoning in Q-Learning}
	\label{alg:meta-rl} 
	\begin{algorithmic}[1]
		\REQUIRE $s, a, R$
		\STATE $\alpha \gets \alpha - (0.05 * R)$ \label{meta-rl:line:meta-level}
		\item[]
		\IF{$\alpha < 0$} \label{meta-rl:line:alpha-begin}
			\STATE $\alpha \gets 0$
		\ENDIF
		\IF{$\alpha > 1$}
			\STATE $\alpha \gets 1$
		\ENDIF \label{meta-rl:line:alpha-end}
		\item[]
		\STATE $Q(s,a) \gets Q(s,a) + (\alpha * R)$ \label{meta-rl:line:qlearning}
	\end{algorithmic}
\end{algorithm}

Meta-level reinforcement learning algorithm requires the same parameters as Q-learning:
a state $s$, an action $a$ and a reward $R$.
In Line~\ref{meta-rl:line:meta-level} we apply the RL update rule for the $\alpha$-value used for the object-level Q-learning algorithm. 
At this point, we are learning the learning-rate, and as we saw, $\alpha$ decreases with positive rewards. 
We use a small constant step of $0.05$ for the meta-level update rule and bound it between $0$ and $1$ (Lines~\ref{meta-rl:line:alpha-begin}--\ref{meta-rl:line:alpha-end}) to ensure it remains a consistent learning-rate value for Q-learning. 
Such a small learning-rate at the meta-level aims to ensure that while we are constantly updating the object-level learning-rate, we avoid high variations. 
Finally, in Line~\ref{meta-rl:line:qlearning} we use the standard update rule for Q-learning,
using the adapted learning-rate. 
As our algorithm is nothing but a short sequence of mathematical operations, it is really efficient when it comes to time. 
Thus, it is able to execute in few clock cycles and could be utilized in real-time after each action execution.



\subsection{Meta-Level Reasoning in Exploration Policy}
\label{subsec:mlep}

Since we are modifying the learning-rate based on the feedback obtained by the agent, and increasing it when the agent detects that its knowledge is no longer up to date, we can also use this value to guide the exploration policy. 
Thus, we also modify the $\epsilon-$greedy action selection algorithm.
Instead of keeping the exploration-rate ($\epsilon-$value) constant, we apply the same meta-level reasoning to the $\epsilon-$value, increasing the exploration-rate, whenever we find that the agent must increase its learning-rate ---  the more the agent wants to learn, the more it wants to explore;
if there is nothing to learn, there is nothing to explore.
To accomplish this, we had first defined the exploration-rate as been always equal to the learning-rate:
\[
	\epsilon \gets \alpha
\]

This solution has proved not to work well, since there are cases when the learning-rate and exploration-rate are maximized in $1$, causing the agent to choose all its actions randomly and preventing the convergence of learning.
When the exploration-rate is near $1$, the agent chooses more random actions than learned actions, which normally makes the agent commit more mistakes, preventing the learning-rate from converging to $0$.
To solve this, we defined the exploration-rate to be half of learning-rate. This makes the highest exploration-rate be $0.5$, when the learning-rate is maximized in $1$, while still maintaining a exploration-rate of $0$ when learning-rate is $0$.
\[
	\epsilon \gets \alpha / 2
\]

Our approach to \textit{meta-level exploration policy} is shown in Algorithm~\ref{alg:meta-exploration}. 

\begin{algorithm}
	\caption{Meta-Level Exploration Policy}
	\label{alg:meta-exploration} 
	\begin{algorithmic}[1]
		\REQUIRE $V_a$ (vector with possible actions), $\alpha$ (learning-rate)
		\STATE $a_{max} \gets \max_{a \in V_a}$ \label{meta-exploration:line:max_a}
		\STATE $r \gets rand(0,1)$ \label{meta-exploration:line:rand}
		\STATE $\epsilon \gets \alpha / 2$ \label{meta-exploration:line:epsilon}
		\IF{$r > \epsilon$} \label{meta-exploration:line:return-begin}
			\RETURN $a_{max}$
		\ELSE
			\RETURN $any(V_a)$
		\ENDIF \label{meta-exploration:line:return-end}
	\end{algorithmic}
\end{algorithm}


Meta-level exploration policy algorithm requires two parameters: learning-rate ($\alpha$) and a vector with all the possible actions ($V_a$).
First, it selects the max-action --- action with highest Q-value for current state --- in Line~\ref{meta-exploration:line:max_a}.
Next it gets a random value between $0$ and $1$ (in Line~\ref{meta-exploration:line:rand}) using it as random factor.
Than, in Line~\ref{meta-exploration:line:epsilon} it calculates the exploration-rate ($\epsilon$).
Finally, between lines~\ref{meta-exploration:line:return-begin} and~\ref{meta-exploration:line:return-end}, if the random factor was higher than exploration-rate, it returns the max-action; else, it just returns any possible action.



\subsection{Meta-Level Reinforcement Learning}
\label{subsec:mlrl}

By using meta-level reasoning, we have improved the classical reinforcement learning.
We use meta-level reasoning in the two approaches --- Q-learning and exploration policy --- together to create \textit{meta-level reinforcement learning}.
With both, we cover the two parts of our problem: \textit{learning} and \textit{exploration}.